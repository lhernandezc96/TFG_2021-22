# -*- coding: utf-8 -*-
"""PFG_v5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HGinRtwDa-efdsjLyjEsHfiwZ4S9cr6r

# Proyecto Fin de Grado: ----título----

## Anexo I: Solución Tecnológica

## Autor: Luis Hernández Casado

## Fecha: 25 de diciembre de 2021

## Introducción

El objetivo del presente documento es reflejar los pasos tomados para la resolución técnica del proyecto de fin de grado planteado. Explicaremos cada una de las fases técnicas en las que se ha dividido el proyecto, así como el código empleado y ejemplos de los los resultados.

El apartado técnico del proyecto se ha dividido en las siguientes fases:

Fase 0.- Utilizando la cámara térmica descrita en otros apartados del proyecto, se ha conseguido un alto número de imágenes de los objetos, aprovechando la funcionalidad que que ofrece esta cámara de presentar la misma imagen tanto en formato estandar como en formato térmico. El conjunto de imágenes obtenidas es de 1.082 en total siendo la mitad (541) imágenes tradicionales y la otra mitad (541) imágenes térmicas tomadas simultáneamente y que por tanto representan la misma imagen que la tradicional, pero en el espectro infrarojo.

Fase 1.- Se usa la aplicación de reconocimiento de imágenes Detectron2 para identificar los objetos en las imágenes estandar y conseguir unas nuevas imágenes en las que los objetos identificados están acotados. Antes de proc eder a este reconocimiento dividiremos las 541 imágenes tradicionales (y en paralelo las imágenes térmicas) en tres grupos:

    - Un primer grupo de 433 imágenes que serán utilizadas posteriormente para el entrenamiento del nuevo modelo de reconocimiento.
    - Un segundo grupo de 54 imágenes que también serán utilizadas en la fase de entrenamiento pero como imágenes de validación del modelo para reforzar el aprendizaje.
    - Un tercer grupo de 54 imágenes que se utilizará para probar la eficiencia del modelo generado

Con esta distribución de imágenes estaremos usando un 80% de las mismas para el entrenamiento y el resto para validación y test.

Fase2.- Las acotaciones obtenidas sobre las imágenes estandar se utilizan para acotar los objetos representados en las imágenes térmicas. Estas nuevas imágenes acotadas se usan para crear el conjunto de entrenamiento del nuevo modelo. Este proceso se realizará tanto para el grupo de imágenes de entrenamiento (433) como para el grupo de imágenes de validación (54) pero no para el grupo de imágenes de prueba que se reservan para la última fase.

Fase 3.- Entrenamiento del nuevo modelo de reconocimiento de imágenes térmicas. Las imágenes térmicas acotadas obtenidas en la fase anterior se utilizan como información de partida para el entrenamiento del nuevo modelo de reconocimiento de imágenes térmicas.

Fase 4.- Prueba del nuevo modelo. Una vez creado el nuevo modelo, se prueba el mismo con el subconjunto de  imágenes térmicas conseguidas en la fase 0 del proyecto. Estas imágenes se reservaron, no siendo utilizadas en la fase de entrenamiento. El objetivo de esta fase es probar el modelo y comprobar su eficacia.

## Preparación del entorno:

Para empezar el desarrollo, el primer paso necesario es la preparación un entorno de programación adecuado con la instalación tanto de Python como de los módulos adicionales necesarios, cada uno de ellos en las versiones adecuadas para poder ejecutar los programas sin errores ni incompatibilidades.

Hemos escogido, para evitar ciertas incompatibilidades, trabajar con la versión 3.6.13 de Python y hemos creado un entorno específico de desarrollo, llamado PFG_env, en el que hemos cargado diferentes módulos cuyo listado es el siguiente:
"""

#conda list
!pip install pyyaml==5.1

import torch
TORCH_VERSION = ".".join(torch.__version__.split(".")[:2])
CUDA_VERSION = torch.__version__.split("+")[-1]
print("torch: ", TORCH_VERSION, "; cuda: ", CUDA_VERSION)
# Install detectron2 that matches the above pytorch version
# See https://detectron2.readthedocs.io/tutorials/install.html for instructions
!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/$CUDA_VERSION/torch$TORCH_VERSION/index.html
# If there is not yet a detectron2 release that matches the given torch + CUDA version, you need to install a different pytorch.

# exit(0)  # After installation, you may need to "restart runtime" in Colab. This line can also restart runtime

"""Entre los módulos descargados podemos destacar como críticos:

* cudatoolkit, v11.1.74, este módulo nos va a permitir utilizar la capacidad de proceso de la GPU Nvidia del ordenador para acelerar el tiempo de los procesos
* detectron2, v0.4+cu111, se trata del módulo básico que vamos a utilizar tanto para el entrenamiento de la red como para el reconocimiento de las imágenes, tanto en sus versiones estandar como térmicas, es la versión adaptada al uso de las capacidades CUDA de multiproceso masivo que facilita la tarjeta gráfica NVIDIA del ordenador utilizado.
* opencv-python, v4.5.2.54, este módulo nos permitirá realizar tanto el tratamiento de la imágenes como gestionar su entrada y salida
* pytorch, v1.8.1 y torchvision, v0.9.1, estos módulos consisten en librerías usadas para la ejecución de programas de deep learning y son requeridos por Detectron 2 para su ejecución
* Otros módulos necesarios para el tratamiento numérico entre los que destacan numpy y pandas


A continuación procedemos a importar a Python los módulos necesarios para la ejecución del código desarrollado:
"""

import torch, torchvision
print("torch ", torch.__version__, torch.cuda.is_available())

# Importamos detectron2 y su logger
import detectron2
from detectron2.utils.logger import setup_logger
setup_logger()
print("detectron2 ", detectron2.__version__)

# Importamos otros módulos y librerias comunes
import numpy as np
import os, json, cv2, random
import pathlib
from google.colab.patches import cv2_imshow

print("numpy ", np.__version__)

"""A continuación importamos varias utilidades necesarias de Detectron2 como el conjunto de modelos predefinidos (model_zoo) y las utilidades de predicción y visualización de resultados."""

from detectron2 import model_zoo

from detectron2.engine import DefaultPredictor

from detectron2.config import get_cfg

from detectron2.utils.visualizer import Visualizer

from detectron2.data import MetadataCatalog, DatasetCatalog

from detectron2.evaluation import COCOEvaluator

from detectron2.engine import DefaultTrainer

class CocoTrainer(DefaultTrainer):

  @classmethod
  def build_evaluator(cls, cfg, dataset_name, output_folder=None):

    if output_folder is None:
        os.makedirs("coco_eval", exist_ok=True)
        output_folder = "coco_eval"

    return COCOEvaluator(dataset_name, cfg, False, output_folder)



"""## Fase 1

Una vez que tenemos el entorno listo vamos a ir ejecutando cada una de las fases en las que se divide el proyecto.

En la primera fase vamos a coger una parte (*) del conjunto de fotografías que hemos realizado y usaremos las imágenes en formato estandar para identificar en ellas los objetos y conseguir unas imágenes modificadas con las acotaciones de los objetos identificados.

Partimos de un directorio donde tenemos almacenadas las imágenes en formato estandar y otro directorio donde están las imágenes de los mismos objetos pero en formato térmico.

Este proceso lo ejecutaremos dos veces, una para las imágenes que vamos a utilizar para el entrenamiento y otra vez para las imágenes de validación.

Como resultado de esta fase obtendremos, para cada conjunto de imágenes (train y valid), un tercer directorio donde se almacenarán las imágenes térmicas junto a sus correspondientes acotaciones, en formato txt, que identifican los objetos reconocidos en cada imagen. Con el contenido de este directorio crearemos, en la siguiente fase, el conjunto de entrenamiento del nuevo modelo.

(*) las imagenes no utilizadas ahora, almacenadas en el directorio TEST, se reservan para la fase de prueba del nuevo modelo

###
"""

from google.colab import drive
drive.mount('/content/drive')

import pathlib
import random

dir_trad = '/content/drive/MyDrive/Colab Notebooks/coches_trad/train/'
dir_term = '/content/drive/MyDrive/Colab Notebooks/coches_term/train/'
dir_MOD_term = '/content/drive/MyDrive/Colab Notebooks/MOD_coches_term/'
directorio = pathlib.Path(dir_trad)

# Para cada una de las imágenes tradicionales ejecutamos el programa de reconocimiento para identificar los
# objetos reconocidos y su posición

for fichero in directorio.iterdir():

    ficherostr=str(fichero)
    im = cv2.imread(ficherostr)

    im_ter = cv2.imread(dir_term + ficherostr[57:65] + ".jpg")
    #print(ficherostr[57:65])
    #print(dir_term + ficherostr[57:65] + ".jpg")

    cfg = detectron2.config.get_cfg()
    cfg.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"))
    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.75  # mínimo de fiabilidad que queremos en los elementos reconocidos
    # Escogemos uno de los modelos de reconocimiento incluidos el modelzoo
    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")

    predictor = DefaultPredictor(cfg)
    outputs = predictor(im)

    # Grabamos, para cada imagen analizada, un fichero de texto en el que salvamos las clases y los marcos
    grabaanot=open(dir_MOD_term + "txt_files/" + ficherostr[57:65] + ".txt","w")
    grabaanot.write(str(outputs["instances"].pred_classes) + "\n")
    grabaanot.write(str(outputs["instances"].pred_boxes))
    grabaanot.close()

    # Usamos el visualizador para dibujar los marcos de los objetos reconocidos en las imágenes tradicionales,
    # pero lo hacemos ya sobre las imágenes térmicas que serán usadas en el entrenamiento posterior del sistema

    v = Visualizer(im_ter[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.0)
    out = v.draw_instance_predictions(outputs["instances"].to("cpu"))

    vtrad = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.0)
    outtrad = vtrad.draw_instance_predictions(outputs["instances"].to("cpu"))

    for box in outputs["instances"].pred_boxes.to("cpu"):
        v.draw_box(box)

    # Generamos la imagen térmica modificada con los marcos y la grabamos en el disco
    im_mod = out.get_image()[:, :, ::-1]
    cv2.imwrite(dir_MOD_term + ficherostr[57:],im_mod)

    #Para un porcentaje del aprox. el 5% de las imágenes vamos a mostrar los resultados
    rndm = random.randint(0,99)
    if rndm > 75:

        print(ficherostr[56:])

        # Mostramos las imágenes tradicional y térmica
        cv2_imshow(im)
        cv2_imshow(im_ter)

        #Imprimimos las clases de objetos reconocidos y los puntos que marcan las cajas donde se enmarcan los
        # objetos que reconoce la aplicación
        print(outputs["instances"].pred_classes)
        print(outputs["instances"].pred_boxes)

        # Visualizamos la imagen térmica con los marcos sobre los objetos reconocidos


        cv2_imshow(outtrad.get_image()[:, :, ::-1])
        cv2_imshow(out.get_image()[:, :, ::-1])

# Reconocimiento de imágenes para el conjunto de validación (valid)

dir_trad = '/content/drive/MyDrive/Colab Notebooks/coches_trad/valid/'
dir_term = '/content/drive/MyDrive/Colab Notebooks/coches_term/valid/'
dir_MOD_term = '/content/drive/MyDrive/Colab Notebooks/MOD_coches_ter2/'
directorio = pathlib.Path(dir_trad)

# Para cada una de las imágenes tradicionales ejecutamos el programa de reconocimiento para identificar los
# objetos reconocidos y su posición

for fichero in directorio.iterdir():

    ficherostr=str(fichero)
    im = cv2.imread(ficherostr)

    im_ter = cv2.imread(dir_term + ficherostr[57:65] + ".jpg")

    cfg = detectron2.config.get_cfg()
    cfg.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"))
    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.75  # mínimo de fiabilidad que queremos en los elementos reconocidos
    # Escogemos uno de los modelos de reconocimiento incluidos el modelzoo
    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")

    predictor = DefaultPredictor(cfg)
    outputs = predictor(im)

    # Grabamos, para cada imagen analizada, un fichero de texto en el que salvamos las clases y los marcos
    grabaanot=open(dir_MOD_term + "txt_files/" + ficherostr[57:65] + ".txt","w")
    grabaanot.write(str(outputs["instances"].pred_classes) + "\n")
    grabaanot.write(str(outputs["instances"].pred_boxes))
    grabaanot.close()

    # Usamos el visualizador para dibujar los marcos de los objetos reconocidos en las imágenes tradicionales,
    # pero lo hacemos ya sobre las imágenes térmicas que serán usadas en el entrenamiento posterior del sistema

    v = Visualizer(im_ter[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.0)
    out = v.draw_instance_predictions(outputs["instances"].to("cpu"))

    vtrad = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.0)
    outtrad = vtrad.draw_instance_predictions(outputs["instances"].to("cpu"))


    for box in outputs["instances"].pred_boxes.to("cpu"):
        v.draw_box(box)

    # Generamos la imagen térmica modificada con los marcos y la grabamos en el disco
    im_mod = out.get_image()[:, :, ::-1]
    cv2.imwrite(dir_MOD_term + ficherostr[57:],im_mod)

    #Para un porcentaje del aprox. el 20% de las imágenes vamos a mostrar los resultados
    rndm = random.randint(0,99)
    if rndm > 80:

        print(ficherostr[56:])

        # Mostramos las imágenes tradicional y térmica
        cv2_imshow(im)
        cv2_imshow(im_ter)

        #Imprimimos las clases de objetos reconocidos y los puntos que marcan las cajas donde se enmarcan los
        # objetos que reconoce la aplicación
        print(outputs["instances"].pred_classes)
        print(outputs["instances"].pred_boxes)

        # Visualizamos la imagen térmica con los marcos sobre los objetos reconocidos
        cv2_imshow(out.get_image()[:, :, ::-1])

        cv2_imshow(outtrad.get_image()[:, :, ::-1])

"""## Fase 2


Antes de proceder al entrenamiento del sistema con las imágenes térmicas necesitamos adaptar la información recogida sobre los objetos reconocidos a un formato estandarizado.

En el caso de Detectron2 éste es el formato COCO, basado en un fichero json que combina en forma de diccionario la información de la delimitación de los objetos reconocidos para todas la imágenes.

Para realizar el trabajo de extraer la información de los ficheros txt y combinarlos en un fichero json adaptado al formato COCO, vamos a crear una nueva función que llamaremos "convert".

Puesto que tenemos dos conjuntos de imágenes con anotaciones (train y valid), usaremos esta función para ambos conjuntos.
"""

INICIO_BOUNDING_BOX_ID = 1

# Sólo usaremos una categoría, por ello cambiaremos la categoría "car" al número 0

CATEGORIES_PRE_DEFINIDAS= {"car": 0}

def convert(txt_files, json_file):
    json_dict = {"images": [], "type": "instances", "annotations": [], "categories": []}
    categories = CATEGORIES_PRE_DEFINIDAS
    bnd_id = INICIO_BOUNDING_BOX_ID
    directorio = pathlib.Path(txt_files)

    texto_completo = ""

    # Para cada imagen añadimos al diccionario json la información necesaria
    for fichero in directorio.iterdir():

        txt = open(fichero, "r")
        texto = txt.read()
        txt.close()

        texto_completo += (texto + "\n")
        image_id = str(fichero)[65:73]

        filename = image_id

        size = "size"
        width = 320
        height = 240
        image = {
            "file_name": filename + ".jpg",
            "height": height,
            "width": width,
            "id": image_id,
        }

        json_dict["images"].append(image)

        nuevo_texto = ""

        index = 0

        # Repasamos el texto para eliminar caracteres de separación y extraer las clases de objetos y las
        # coordenadas de los marcos de las imágenes
        while index < len(texto):
            char = texto[index]

            if char == "[":

                while char != "]":
                    nuevo_texto += char
                    index += 1
                    char = texto[index]
                nuevo_texto += "],"

            index += 1

        nuevo_texto = nuevo_texto.replace("]," , " * ")
        nuevo_texto = nuevo_texto.replace("]" , "")
        nuevo_texto = nuevo_texto.replace("[" , "")
        nuevo_texto = nuevo_texto.replace(" " , "")
        nuevo_texto = nuevo_texto[:-1]

        nueva_lista = nuevo_texto.split("*")

        lista_anot = []

        for item in nueva_lista:
            item = str(item).split(",")
            lista_anot.append(item)

        for item in lista_anot[0]:
            # Para concentrarnos en el entrenamiento para reconocer coches en formato térmico, eliminamos otras
            # posibles categorías reconocidas en las imágenes tradicionales
            if item != "2":
                x = lista_anot[0].index(item)
                lista_anot[0].pop(x)
                lista_anot.pop(x+1)

        for item in lista_anot[1:]:
            # Extraemos la información de los dos vértices de los marcos para guardarlo en el formato de
            # un vértice más altura y anchura


            xmin = round(float(item[0]))
            ymin = round(float(item[1]))
            xmax = round(float(item[2]))
            ymax = round(float(item[3]))
            o_width = abs(xmax - xmin)
            o_height = abs(ymax - ymin)

            ann = {
                "area": o_width * o_height,
                "iscrowd": 0,
                "image_id": image_id,
                "bbox": [xmin, ymin, o_width, o_height],
                "category_id": 0,
                "id": bnd_id,
                "ignore": 0,
                "segmentation": [],
            }

            json_dict["annotations"].append(ann)
            bnd_id = bnd_id + 1

# Grabamos como categoría 0 todas las detecciones de "car"

    # Añadimos la información requerida de categorías reconocidas
    for cate, cid in CATEGORIES_PRE_DEFINIDAS.items():
        cat = {"supercategory": "none", "id": cid, "name": cate}
        json_dict["categories"].append(cat)


    json_fp = open(json_file, "w")
    json_str = json.dumps(json_dict)
    json_fp.write(json_str)
    json_fp.close()



convert('/content/drive/MyDrive/Colab Notebooks/MOD_coches_term/txt_files', '/content/drive/MyDrive/Colab Notebooks/MOD_coches_term/converted_json.json')

convert('/content/drive/MyDrive/Colab Notebooks/MOD_coches_ter2/txt_files', '/content/drive/MyDrive/Colab Notebooks/MOD_coches_ter2/converted_json.json')

# Procedemos a registrar en Detectron2 los datasets que hemos creado para entrenamiento y validación

from detectron2.data.datasets import register_coco_instances

register_coco_instances("train1", {}, "/content/drive/MyDrive/Colab Notebooks/MOD_coches_term/converted_json.json", "/content/drive/MyDrive/Colab Notebooks/coches_term/train/")

register_coco_instances("valid1", {}, "/content/drive/MyDrive/Colab Notebooks/MOD_coches_ter2/converted_json.json", "/content/drive/MyDrive/Colab Notebooks/coches_term/valid/")

# Con objeto de visualizar los resultados creamos un diccionario de los datos de entrenamiento y sus metadatos

sample_metadata = MetadataCatalog.get("train1")
dataset_dicts = DatasetCatalog.get("train1")

# Visualizamos los metadatos de entrenamiento y de validación

print (sample_metadata)

print(MetadataCatalog.get("valid1"))

# Comprobamos, visualizando varias imágenes, que las anotaciones y metadatos que hemos creado identifican en las
# fotos térmicas los objetos detectados

for d in random.sample(dataset_dicts, 4):

    img = cv2.imread(d["file_name"])
    print (d["file_name"])

    visualizer = Visualizer(img[:, :, ::-1], metadata=sample_metadata, scale=1.5)
    vis = visualizer.draw_dataset_dict(d)
    cv2_imshow(vis.get_image()[:, :, ::-1])

"""## Fase 3


Procedemos al entrenamiento del modelo utilizando para ello los datasets que hemos creado de entrenamiento y validación, Puesto que en nuestros metadatos tenemos identificados los marcos de las imágenes reconocidas y no las siluetas, vamos a utilizar para el entrenamiento uno de los modelos adecuados para ello dentro del "model-zoo" llamado: faster_rcnn_R_50_FPN_3x.yaml
"""

# Iniciamos la fase de entrenamiento. Debido a un problema de incompatibilidad la ejecución rápida usando la
# potencia de la GPU da problemas por lo que se usará la propia CPU


from detectron2.engine import DefaultTrainer
from detectron2.config import get_cfg

cfg = get_cfg()

#cfg.MODEL.DEVICE = "cpu"

cfg.merge_from_file(model_zoo.get_config_file("COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml"))
cfg.DATASETS.TRAIN = ("train1",)
cfg.DATASETS.TEST = ("valid1",)

cfg.DATALOADER.NUM_WORKERS = 4
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml")  # Let training initialize from model zoo
cfg.SOLVER.IMS_PER_BATCH = 3
cfg.SOLVER.BASE_LR = 0.001


cfg.SOLVER.WARMUP_ITERS = 1000
cfg.SOLVER.MAX_ITER = 1501 #adjust up if val mAP is still rising, adjust down if overfit
cfg.SOLVER.STEPS = (1000, 1500)
cfg.SOLVER.GAMMA = 0.05




#cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 64
cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 254
cfg.MODEL.ROI_HEADS.NUM_CLASSES = 80 #your number of classes + 1

#cfg.TEST.EVAL_PERIOD = 500
cfg.TEST.EVAL_PERIOD = 100

os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)
trainer = CocoTrainer(cfg)
trainer.resume_or_load(resume=False)
trainer.train()



"""
cfg = get_cfg()

cfg.MODEL.DEVICE = "cpu"

cfg.merge_from_file(model_zoo.get_config_file("COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml"))
cfg.DATASETS.TRAIN = ("train",)
cfg.DATASETS.TEST = ("valid",)
cfg.DATALOADER.NUM_WORKERS = 2
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml")
cfg.SOLVER.IMS_PER_BATCH = 2
cfg.SOLVER.BASE_LR = 0.002
cfg.SOLVER.MAX_ITER = 150
cfg.SOLVER.STEPS = []
cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128
cfg.MODEL.ROI_HEADS.NUM_CLASSES = 80


os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)
trainer = DefaultTrainer(cfg)
trainer.resume_or_load(resume=True)
trainer.train()
"""





"""A continuación vamos a ejecutar el módulo de evaluación de Detectron2, sobre el conjunto de datos de validación para extraer información sobre la calidad de los resultados y del modelo"""

# Utilizaremos para la evaluación los parametros del modelo que hemos creado en la fase de entrenamiento:
# model_final.pth

#cfg.MODEL.DEVICE = "cpu"

cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, "model_final.pth")
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # threshold de reconocimiento
predictor = DefaultPredictor(cfg)

from detectron2.evaluation import COCOEvaluator, inference_on_dataset
from detectron2.data import build_detection_test_loader

evaluator = COCOEvaluator("valid1", output_dir="./output")
val_loader = build_detection_test_loader(cfg, "valid1")

cfg.MODEL.DEVICE = "cuda"
print(inference_on_dataset(predictor.model, val_loader, evaluator))

from detectron2.utils.visualizer import ColorMode

dataset_dicts = DatasetCatalog.get("valid1")
#print(dataset_dicts)

for d in random.sample(dataset_dicts, 20):
    im = cv2.imread(d["file_name"])
    outputs = predictor(im)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format
    v = Visualizer(im[:, :, ::-1],
                   metadata=sample_metadata,
                   scale=2,
                   )
    out = v.draw_instance_predictions(outputs["instances"].to("cpu"))
    cv2_imshow(out.get_image()[:, :, ::-1])

# Finalmente veremos el resultado de aplicar el modelo de reconocimiento a las 54 imágenes que habiamos
# reservado originalmente para hacer la prueba del modelo


cfg.MODEL.DEVICE = "cuda"

dir_trad = '/content/drive/MyDrive/Colab Notebooks/coches_trad/'
dir_term = '/content/drive/MyDrive/Colab Notebooks/coches_term/TEST0/'

directorio = pathlib.Path(dir_term)


# Para cada una de las imágenes térmicas ejecutamos el programa de reconocimiento para identificar los
# objetos reconocidos y su posición

for fichero in directorio.iterdir():

    ficherostr=str(fichero)

    im = cv2.imread(ficherostr)

    im_trad = cv2.imread(dir_trad + ficherostr[57:65] + "- fotografía.jpg")
    print(dir_trad + ficherostr[57:65] + "- fotografía.jpg")

    cfg = detectron2.config.get_cfg()

    cfg.merge_from_file(model_zoo.get_config_file("COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml"))
    cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, "model_final.pth")  # path to the model we just trained
    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.90   # set a custom testing threshold
    predictor = DefaultPredictor(cfg)

    outputs = predictor(im)



    v = Visualizer(im[:, :, ::-1], metadata=sample_metadata, scale=1.0)

    out = v.draw_instance_predictions(outputs["instances"].to("cpu"))

    vtrad = Visualizer(im_trad[:, :, ::-1], metadata=sample_metadata, scale=1.0)

    outtrad = vtrad.draw_instance_predictions(outputs["instances"].to("cpu"))


    for box in outputs["instances"].pred_boxes.to("cpu"):
        v.draw_box(box)

    #Para un porcentaje del aprox. el 50% de las imágenes vamos a mostrar los resultados


        print(ficherostr[56:])

        # Mostramos las imágenes tradicional y térmica
        #cv2_imshow(im)
        cv2_imshow(im_trad)
        #print (type(im))

        #Imprimimos las clases de objetos reconocidos y los puntos que marcan las cajas donde se enmarcan los
        # objetos que reconoce la aplicación
        print(outputs["instances"].pred_classes)
        print(outputs["instances"].pred_boxes)

        # Visualizamos la imagen térmica con los marcos sobre los objetos reconocidos
        cv2_imshow(out.get_image()[:, :, ::-1])
        cv2_imshow(outtrad.get_image()[:, :, ::-1])

# Commented out IPython magic to ensure Python compatibility.
# Look at training curves in tensorboard:
# %load_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir output

"""[blue_text](http://localhost:6007/#scalars&_smoothingWeight=0.6)"""

cfg = get_cfg()
cfg.DATASETS.TEST = ("/content/drive/MyDrive/Colab Notebooks/coches_term/valid/",)
cfg.TEST.EVAL_PERIOD = 100

class MyTrainer(DefaultTrainer):
    @classmethod
    def build_evaluator(cls, cfg, dataset_name, output_folder=None):
        if output_folder is None:
            output_folder = os.path.join(cfg.OUTPUT_DIR,"inference")
        return COCOEvaluator(dataset_name, cfg, True, output_folder)

def build_hooks(self):
        hooks = super().build_hooks()
        hooks.insert(-1,LossEvalHook(
            cfg.TEST.EVAL_PERIOD,
            self.model,
            build_detection_test_loader(
                self.cfg,
                self.cfg.DATASETS.TEST[0],
                DatasetMapper(self.cfg,True)
            )
        ))
        return hooks

from detectron2.engine.hooks import HookBase
from detectron2.evaluation import inference_context
from detectron2.utils.logger import log_every_n_seconds
from detectron2.data import DatasetMapper, build_detection_test_loader
import detectron2.utils.comm as comm
import torch
import time
import datetime
class LossEvalHook(HookBase):
    def __init__(self, eval_period, model, data_loader):
        self._model = model
        self._period = eval_period
        self._data_loader = data_loader

    def _do_loss_eval(self):
        # Copying inference_on_dataset from evaluator.py
        total = len(self._data_loader)
        num_warmup = min(5, total - 1)

        start_time = time.perf_counter()
        total_compute_time = 0
        losses = []
        for idx, inputs in enumerate(self._data_loader):
            if idx == num_warmup:
                start_time = time.perf_counter()
                total_compute_time = 0
            start_compute_time = time.perf_counter()
            if torch.cuda.is_available():
                torch.cuda.synchronize()
            total_compute_time += time.perf_counter() - start_compute_time
            iters_after_start = idx + 1 - num_warmup * int(idx >= num_warmup)
            seconds_per_img = total_compute_time / iters_after_start
            if idx >= num_warmup * 2 or seconds_per_img > 5:
                total_seconds_per_img = (time.perf_counter() - start_time) / iters_after_start
                eta = datetime.timedelta(seconds=int(total_seconds_per_img * (total - idx - 1)))
                log_every_n_seconds(
                    logging.INFO,
                    "Loss on Validation  done {}/{}. {:.4f} s / img. ETA={}".format(
                        idx + 1, total, seconds_per_img, str(eta)
                    ),
                    n=5,
                )
            loss_batch = self._get_loss(inputs)
            losses.append(loss_batch)
        mean_loss = np.mean(losses)
        self.trainer.storage.put_scalar('validation_loss', mean_loss)
        comm.synchronize()

        return losses

    def _get_loss(self, data):
        # How loss is calculated on train_loop
        metrics_dict = self._model(data)
        metrics_dict = {
            k: v.detach().cpu().item() if isinstance(v, torch.Tensor) else float(v)
            for k, v in metrics_dict.items()
        }
        total_losses_reduced = sum(loss for loss in metrics_dict.values())
        return total_losses_reduced


    def after_step(self):
        next_iter = self.trainer.iter + 1
        is_final = next_iter == self.trainer.max_iter
        if is_final or (self._period > 0 and next_iter % self._period == 0):
            self._do_loss_eval()
        self.trainer.storage.put_scalars(timetest=12)

class MyTrainer(DefaultTrainer):
    @classmethod
    def build_evaluator(cls, cfg, dataset_name, output_folder=None):
        if output_folder is None:
            output_folder = os.path.join(cfg.OUTPUT_DIR, "inference")
        return COCOEvaluator(dataset_name, cfg, True, output_folder)

    def build_hooks(self):
        hooks = super().build_hooks()
        hooks.insert(-1,LossEvalHook(
            cfg.TEST.EVAL_PERIOD,
            self.model,
            build_detection_test_loader(
                self.cfg,
                self.cfg.DATASETS.TEST[0],
                DatasetMapper(self.cfg,True)
            )
        ))
        return hooks

import json
import matplotlib.pyplot as plt

experiment_folder = '/output'

def load_json_arr(json_path):
    lines = []
    with open(json_path, 'r') as f:
        for line in f:
            lines.append(json.loads(line))
    return lines

experiment_metrics = load_json_arr('/content/output/metrics.json')

plt.plot(
    [x['iteration'] for x in experiment_metrics],
    [x['total_loss'] for x in experiment_metrics])
plt.plot(
    [x['iteration'] for x in experiment_metrics if 'validation_loss' in x],
    [x['validation_loss'] for x in experiment_metrics if 'validation_loss' in x])
plt.legend(['total_loss', 'validation_loss'], loc='upper left')
plt.show()